---
title: "Paper Review: GloVe - Global Vectors for Word Representation"
layout: post
math: true
date: 2018-12-26
categories: NLP DeepLearning PaperReview
permalink: nlp/deeplearning/paperreview/Glove/
comments: true

---


이 포스트에서는 GloVe 논문을 분석한다. 최근 주목받는 Word 또는 Character Embedding의 삼대장은 [Word2Vec]({{site.baseurl}}/nlp/deeplearning/paperreview/Paper-Review-Distributed-Representations-ofWords-and-Phrases-and-their-Compositionality/), GloVe, FastText인 듯하다. [Ratsgo 블로그 포스트](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/11/embedding/)에서도 이 세 가지 임베딩을 집중적으로 다루고 있다. 그런데 최근 작성하고 있는 [포스트]({{site.baseurl}}/nlp/deeplearning/paperreview/Recent-Trends-in-Deep-Learning-Based-Natural-Language/)에서 GloVe를 언급하기는 하는데 자세히 다루지는 않았다. 왜 3대장인데 언급한번하고 끝나지 하고 원논문을 읽어보았는데, 이유는 간단했다. **GloVe는 신경망 기반 기법이 아니기 때문.**

그렇다면 Distributed Representation을 만드는데 신경망을 기반으로 하지 않는 기법이 GloVe 이전에도 있었는가? 맞다. Global Matrix Factorization 기반의 Latent Semantic Analysis (LSA) 가 바로 그러한 예다. 오늘 다루고자 하는 논문은 Global Matrix Factorization 기법과 Word2Vec 등이 포함되는 Local Context Window 기법의 장점을 합친 기법인 **GloVe: Global Vectors for Words Representation 이라는 제목의 논문**이다.

## 논문의 목적

[Word2Vec]({{site.baseurl}}/nlp/deeplearning/paperreview/Paper-Review-Distributed-Representations-ofWords-and-Phrases-and-their-Compositionality/)는 여러가지 관점에서 혁신적인 논문이었다. 그 여러가지 혁신 중 이 논문에서 주목한 것은 Word2Vec의 벡터 산술 (Vector Arithmetic) 관련 성질이다.

### 벡터 산술이란

개인적으로 벡터 산술은 Word2Vec이 그 이전의 기법보다 뛰어난 Distributed Representation 임을 아주 직관적으로 보여주는 증거라고 생각한다. Word2Vec가 보여주는 대표적인 벡터 산술의 예제를 보자.

$$word2vec(`` king") - word2vec(``man") + word2vec(``woman") \sim word2vec(``queen")$$

왕에서 남성적인 요소를 모두 지워버리고 여성적인 요소를 새로 부여하면 여왕의 개념과 부합한다는 점에서 설득력있는 Word Representation이다. 이 예제는 단어의 의미적 규칙 (semantic regularity)을 저차원 (e.g. 300차원) 실수 공간에서의 벡터 산술로 재현해냈다는 점에서 매우 큰 의의가 있다. 의미적 규칙을 산술연산으로 재현해내는 것은 기존의 Distributed Representation (e.g. LSA)에서는 불가능한 영역이었다.

Word2Vec는 단어의 의미적 규칙외에도 문법적 규칙(syntactic regularity) 또한 재현해낼 수 있다. syntactic이 사전적으로 '통어적'이라고 하는데 말이 너무어려워서 '문법적'이라고 번역하겠다. 이제 아래 예제를 살펴보자.

$$word2vec(``dancing") - word2vec(``dance") + word2vec(``fly") \sim word2vec(``flying")$$

이번에는 dancing에서 dance를 빼고 fly를 더했다. dancing과 dance는 의미적으로 "춤추다"라는 뜻을 가지고 있는 것은 같으나 동명사냐 명사냐의 차이만을 가진다. 이 차이, 즉 동명사냐 명사냐의 차이를 동사인 fly에게 더해주면 flying이라는 결과가 나온다.  이 예제는 단어의 문법적 규칙이 저차원 실수 공간에서 벡터 산술로 재현해내었다는 것을 보여준다. 이 역시 기존의 Distributed Representation (e.g. LSA)에서는 불가능한 영역이었다.

### 그래서 이 논문의 목적은

Word2Vec은 신기하게도 이러한 단어간의 규칙을 벡터 연산으로 표현하는 것에 성공했다. 그러나 저자진에 따르면 그 원리가 명확하게 규명된 적은 없었다고 한다.

이 논문의 목적은 이러한 의미적, 문법적 규칙을 단어의 vector공간에서 재현하는 명확한 모델을 만들고 분석하는 것이다.

## 기존 기법 및 문제

이 논문에서는 단어의 Distributed Representation을 위한 기존 기법을 두 가지로 나누어 분석하였다.

(1) Matrix Factorization

첫 번째는 LSA에서 파생된 Matrix Factorization Method 군이다. 이 기법들은 단어-문서 또는 단어-단어 Matrix를 만들어놓고 이 Matrix를 여러 개 ( e.g. 2개)의 행렬로 분해(Decomposition)한다. 이 과정에서 저차원의 공간에서의 단어 Distributed Representation이 가능하다. 이러한 기법은 단어에 대한 전체적인 (global) 통계정보를 활용한다는 점은 강점이나, **단어 유추 문제 (단어에서 나타나는 규칙성을 벡터 공간에서 재현할 수 있는가를 평가하는 문제)에 좋지 않은 성능을 보인다.**

(2) Shallow Window-Based Method

(1)번 처럼 전체적인 단어 통계 정보를 이용하지는 않지만 지역적(local)인 문맥(context) 정보를 한정적으로 사용하여 단어를 vector 형태로 표현하는 방법이 있다. 우리가 이전에 다룬 [Word2Vec]({{site.baseurl}}/nlp/deeplearning/paperreview/Paper-Review-Distributed-Representations-ofWords-and-Phrases-and-their-Compositionality/)의 경우도 한정된 window 크기 내의 단어가 주어졌을 때 중간 단어를 예측하는 과정이나 (CBoW: Continuous  Bag of Words) 반대로 중간 단어가 주어졌을 때 그 주변의 window 크기 내의 단어들을 예측하는 과정 (skip-gram)을 통해 단어를 벡터화하였다. 이 기법은 단어 유추 문제에서는 비교적 좋은 성능을 보이나 **학습 데이터(corpus)에서 관찰되는 단어사용 통계정보를 활용하지 않는 다는 점에서 한정적이다**. 이들은 지역적인 문맥에 대한 학습은 가능하지만, 학습 데이터(corpus)에서 관찰되는 서로 다른 두 단어의 동시발생 횟수 (co-occurrence)에 기반한 학습은 할 수 없기 때문이다.

## 단어사용 통계정보에서 단어의 의미를 추출할 수 있는가?

그렇다면 드는 의문은 이것이다.

>  Word2Vec는 단어의 의미를 충분히 잘 잡아내고 있지 않은가? Word2Vec가 단어사용 통계정보를 활용하지 않는게 문제라고 했는데, 단어사용 통계정보를 활용하면 더 좋아지는가? 단어사용 통계정보에서 단어의 의미를 잡아내는 embedding을 만들어낼 수 있는가?

이 논문은 이 질문에서 시작한다. 이 질문에 답을 하기 위해 다음 예제를 살펴보자.


| Probability and Ratio  | *k = solid*  |  *k = gas*  | *k = water* | *k = fashion* |
|---|---|---|---|---|
| P(k\|ice)| 1.9 x 10^-4 | 6.6 x 10^-5 | 3.0 x 10^-3 | 1.7 x 10^-4 |
| P(k\|steam)| 2.2 x 10^-5 | 7.8 x 10^-4 | 2.2 x 10^-3 | 1.8 x 10 ^-5 |
| P(k\|ice)/P(k\|steam) | 8.9 | 8.5 x 10 ^-2| 1.36| 0.96 |

우리의 관심사는 `ice`라는 단어와 `steam`이라는 단어이다. 두 단어는 각각 물의 열역학적인 상태를 나타내는 단어이다.
먼저, 우리는 두 단어의 차이점을 분석해보기 위해, 두 단어가 다른 단어와 어떤 관계를 가지고 있는지 분석해볼 것이다.
단어 `i`가 사용된 문맥에서 단어 `k`도 사용되었을 확률을 P(k\|i)로 표현해보자.

- 첫 번째 행은 `ice`가 사용된 문맥에서, 단어 `k`도 사용되었을 확률이 나열되어 있다.
- 두 번째 행은 `steam`이 사용된 문맥에서, 단어 `k`도 사용되었을 확률이 나열되어 있다.
- 마지막 행은  P(k\|ice)/P(k\|steam)를 나열했다. 즉 마지막 행은 첫 번째 행의 모든 element를 두 번째 행의 모든 element로 나누어 만들어진 행이다.

그런데 마지막 행, 즉 P(k\|ice)/P(k\|steam) 값을 찬찬히 살펴보니, 어떤 값들은 1에 굉장히 가깝다. `water`와 `fashion`이 그렇다.
결론부터 말하자면, 이 두 단어는 `steam`과 `ice`를 구분 짓는 데에 별 기여를 하지 못하는 단어들이다. 이유는 다음과 같다.


- `fashion`의 경우 P(k\|ice)/P(k\|steam) 값이 1에 가까운 0.96 나왔다. 이는 `ice`가 사용된 문맥에서 `fashion`이 사용될 확률이나, `steam`이 사용된 문맥에서 `fashion`이 사용될 확률이나 별반 차이 없다는 뜻이다. 사실 언뜻 생각해보아도 `fashion`이라는 단어는 `ice`와 `steam`과 그렇게까지 긴밀한 관계를 가지고 있는 단어는 아니지 않는가? `fashion`은 `ice`와도, `steam`과도 별 관계없는 제3의 단어로 볼 수 있다.
- `water`의 경우도 P(k\|ice)/P(k\|steam) 값이 1에 가까운 1.36이다. `steam`이 사용된 문맥에서 `water`가 사용될 확률이나 `ice`가 사용된 문맥에서 `water`가 사용될 확률이 둘 다 높긴한데, 그 수치 또한 비슷비슷 했던 것이다. 사실 `water`는 `ice`와 `steam`과 마찬가지로 물의 열역학적인 상태를 나타내는 (i.e., 얼음->물->증기) 단어다. 두 단어 모두에 밀접한 관련을 맺고 있는 단어이긴 하지만, 그렇다고 `ice`와 `steam`을 구분짓는 데에 결정적인 역할은 수행하는 것은 아니다. 각 단어와 관련 있는 정도가 비슷하기 때문이다.


반면 `solid`와 `gas`의 경우 P(k\|ice)/P(k\|steam) 값이 1보다 한참 아래이거나 한참 위이다. 이 둘은 `ice`와 `steam`를 구분 짓는 데에 큰 기여를 할 수 있는 단어이다. 이유는 다음과 같다.

- `solid`의 경우 P(solid\|ice)/P(solid\|steam)가 무려 8.9이다. `ice`가 `solid`, 즉 고체 상태니까 이 두 단어가 동시사용될 확률이 높다는 것은 그리 놀라운 일이 아니다.
- `gas`의 경우 P(gas\|ice)/P(gas\|steam)의 값은 0.085로 1보다 한참 0에 가까운 값이다. `steam`이 `gas` 상태임을 생각해보면, 이 또한 합리적인 결과이다.


위에서 알 수 있는 사실은, (1) 두 단어와 다른 단어 사이의 관계를 살펴봄으로써 두 단어의 차이를 규명할 수 있으며, (2) 이때 큰 역할을 수행하는 단어도 있고, 별 기여를 못하는 단어도 있다는 점이다.


> 쉬어가는 코너로, 다른 예제를 들어보자. JYP 여자 아이돌 그룹 `트와이스`와 SM 남자 아이돌 그룹 `엑소`를 구분짓는 데에 사용되는 단어는 `남자`, `여자`, `JYP`, `SM` 이지 `아이돌`이나 `그룹`이 아니다. 물론 이 두 단어는 트와이스나 엑소를 설명하는 데에는 매우 큰 역할을 하는 단어이다. 그러나 그 두 그룹을 판가름할 수 있는 핵심어는 따로 있는 셈이다.


이 예제가 시사하는 바는 간단하다. P(solid\|ice)나 P(solid\|steam)와 같은 확률값 자체보다 그 둘 간의 비율, 즉  P(solid\|ice)/P(solid\|steam)이 우리에게 더욱 풍부한 정보를 제공한다는 것이다. 이 비율을 공략하여 Word Vector Learning을 수행하는 것이 본 논문인 GloVe되시겠다.


## 제안 기법 GloVe

만약 P(solid\|ice)나 P(solid\|steam)를 추정할 수 있는 모델이 있다면 어떨까? 만약 이 가상의 모델이 P(wk\|wi)나 P(wk\|wj)를 정확하게 계산해낸다고 가정해보자. 그렇다면 이 모델은 임의의 두 단어인 wi와 wj가 가지는 의미적 차이를 꽤나 잘 알고 있다고 볼 수 있지 않을까?
